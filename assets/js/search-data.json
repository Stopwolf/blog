{
  
    
        "post0": {
            "title": "Recreating Cyclical Learning Rates",
            "content": ". When it comes to neural networks and learning rates, two approaches were always dominating (second one more than the other). Either select a certain learning rate and stick with it, or select a certain learning rate and monotonically decrease it during the training process. . Even though the second approach proved beneficial, it can be difficult to set up optimal starting $l_r$ and schedule for its annealing. . Cyclical learning rates (CLR) provide a solution as they fluctuate $l_r$ between two values at essentially no additional costs and provide a way to select those values optimally, instead of guessing. . I Understanding the Idea . Author of the paper, Leslie Smith, introduces the idea that fluctuating learning rates during training could have, at worst, near-optimal results compared to previously mentioned approaches. . Cyclical Learning Rates could be best understood with the following picture, which is taken directly from the paper: . . Maximum and base learning rates are self explanatory. Stepsize (or half-cycle) is the number of iterations needed for the learning rate (blue line) to reach maximum value. Thus, one full cycle is when our learning rate goes from base to maximum value, and then back again to the base value. . Formally, cyclical learning rates are nothing more than following three equations: $$ cycle = lfloor{(1 + frac{iter}{2 cdot stepsize})} rfloor x = mid frac{iter}{stepsize} - 2 cdot cycle + 1 mid l_r = l_r^{base} + (l_r^{max} - l_r^{base}) cdot max(0, 1-x) $$ . cycle variable gives us the number of the cycle (starting from 1) | x variable fluctuates between 1 and 0 and is the variable responsible for the cyclical part of the name Cyclical Learning Rates | Finally, $l_r$ is the learning rate that we provide to the model in the end | Let&#39;s see the code example: . def clr(base_lr, max_lr, n_iter, stepsize, history): cycle = np.floor(1 + n_iter / (2*stepsize)) x = np.abs(n_iter/ stepsize - 2*cycle + 1) lr = base_lr + (max_lr - base_lr) * np.maximum(0, 1-x) if n_iter % 100 == 0: history[&#39;iter&#39;].append(n_iter) history[&#39;lr&#39;].append(lr) history[&#39;x&#39;].append(x) . With the following piece of code, we can perform one full cycle and then plot our variables&#39; change over time: . history = {&#39;iter&#39; : [], &#39;lr&#39; : [], &#39;x&#39; : []} total_iters = 2001 for i in range(0, total_iters): clr(base_lr=0.01, max_lr=0.05, n_iter=i, stepsize=1000, history=history) . II CLR as a Keras Callback . Since we&#39;ll compare actual neural networks&#39; performance with and without CLR, and Keras library doesn&#39;t possess CLR, we&#39;ll need to create a new Keras Callback. At the end of the post, I linked an official tutorial on custom Keras Callbacks, but the main idea is that we can create different methods which will be invoked when: . a new epoch or batch starts or ends, or | training, evaluation or prediction starts or ends | . When the method will be invoked, solely depends on the method&#39;s name. . In our case, our CyclicalLR Callback class will have: . clr which is the main part of the callback and used to set a new $l_r$ value | on_train_begin which will set the learning rate to the base value | on_batch_end which will increase the number of iterations and invoke clr method | on_epoch_end which will add new info to the history dictionary | . class CyclicalLR(keras.callbacks.Callback): def __init__(self, base_lr, max_lr, stepsize, mode=&#39;triangular&#39;, gamma=1): super(CyclicalLR, self).__init__() self.base_lr = base_lr self.max_lr = max_lr self.stepsize = stepsize self.mode = mode self.gamma = gamma self.iterations = 0 self.history = {&#39;lr&#39; : [], &#39;iter&#39; : []} def clr(self): cycle = np.floor(1 + self.iterations / (2*self.stepsize)) x = np.abs(self.iterations / self.stepsize - 2*cycle + 1) lr = self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, 1-x) if self.mode == &#39;triangular&#39;: return lr elif self.model == &#39;triangular2&#39;: return base_lr + (max_lr - base_lr) * np.maximum(0, 1-x) / (2 ** (cycle-1)) elif self.model == &#39;exp_range&#39;: return base_lr + (max_lr - base_lr) * np.maximum(0, 1-x) * self.gamma ** self.iterations def on_train_begin(self, logs=None): logs = logs or {} if self.iterations == 0: K.set_value(self.model.optimizer.lr, self.base_lr) else: K.set_value(self.model.optimizer.lr, self.clr()) def on_batch_end(self, batch, logs=None): logs = logs or {} self.iterations += 1 K.set_value(self.model.optimizer.lr, self.clr()) def on_epoch_end(self, epoch, logs=None): logs = logs or {} self.history[&#39;lr&#39;].append(float(self.model.optimizer.lr)) self.history[&#39;iter&#39;].append(self.iterations) . You probably noticed there are several different modes of CLRs: triangular, triangular2 and exp_range. We&#39;ll go into further details about these in the VI part. . III Recreating Results on CIFAR-10 . CIFAR-10 is one of the standard datasets for testing new approaches and setting state-of-the-art results. It contains 60,000 low-resolution pictures of 10 classes (airplane, automobile, bird, cat, dog, deer, frog, horse, ship, and truck), of which 10,000 is left out as a test set. . We&#39;ll use this dataset to compare models, but firstly let&#39;s see a few example pictures: . And here&#39;s our simple Keras model: . def build_model(): model = tf.keras.models.Sequential() model.add(tf.keras.layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, input_shape=(32, 32, 3))) model.add(tf.keras.layers.MaxPooling2D((2, 2))) model.add(tf.keras.layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;)) model.add(tf.keras.layers.MaxPooling2D((2, 2))) model.add(tf.keras.layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;)) model.add(tf.keras.layers.Flatten()) model.add(tf.keras.layers.Dense(64, activation=&#39;relu&#39;)) model.add(tf.keras.layers.Dense(10)) return model . IV LR Range Test . IV.I How do we set up base and max learning rate? . Before we begin training our model, we need to answer this question. . LR range test is an approach proposed in the paper for setting up these values. Luckily, the LR Range test can be performed with our CyclicalLR class by setting stepsize the same as the total number of iterations. This way, we&#39;ll have $l_r$ go only once from base to the maximum value (we&#39;ll perform one half-cycle). Afterward, we plot accuracies versus learning rates and select base and maximum learning rates from that plot. . First step would be to calculate total number of iterations. The following equation is really helpful in doing this: $$ I_t = frac {|X|}{n_{bs}} * n_e $$ . $I_t$ is total number of iterations | $|X|$ is number of images in the train set (50,000 in our case) | $n_{bs}$ is batch size (in our case it&#39;ll be 100) | $n_e$ is number of epochs (in our case it&#39;ll be 150) | . This means we&#39;ll now easily get: $$ I_t = frac{50.000}{100} * 150 = 75.000 $$ . Now we initialize our callback as follows and then plot accuracies versus learning rates: . clr_triangular = CyclicalLR(base_lr=0.001, max_lr=0.02, stepsize=75000) . We select the base as the point where the accuracy starts to sharply increase, and the maximum up to the point where it slows or starts to fall. From our example above, we&#39;ll select 0.001 as the base learning rate, and 0.005 as the maximum one. . IV.II How do we set up stepsize parameter? . Paper proposes we set it as 2-8 times the number of iterations in one epoch. That&#39;s why we first need to calculate the number of iterations per epoch with a similar formula to the one we saw before: $$ I = frac {|X|}{n_{bs}} rightarrow frac{50.000}{100}=500$$ . In our case, the model will have 500 iterations per epoch. . Now, stepsize should be in 1000-4000 range. The author says that when comparing 2*stepsize and 8*stepsize, the latter is &quot;only slightly better&quot;. . V Comparison . We&#39;ll first run the model without CLR for the total of 75,000 iterations: . model = build_model() model.compile(optimizer=&#39;adam&#39;, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[&#39;accuracy&#39;]) history = model.fit(train_images, train_labels, epochs=150, batch_size=100, validation_data=(test_images, test_labels)) . Without CLR Loss: 4.367868900299072 Accuracy: 0.6783000230789185 . But, let&#39;s see the same model trained for the third of the iterations, and with CLR: . Note: We now train the model for the third of iterations compared to before, since we want to prove the claim that CLR gives better results faster. . clr_triangular = CyclicalLR(base_lr=0.001, max_lr=0.005, stepsize=2500) model = build_model() model.compile(optimizer=&#39;adam&#39;, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[&#39;accuracy&#39;]) history = model.fit(train_images, train_labels, epochs=50, batch_size=100, validation_data=(test_images, test_labels), callbacks=[clr_triangular]) . With CLR Loss: 2.375159740447998 Accuracy: 0.6941999793052673 . The difference in accuracy may be only ~1.6%, but we did prove the main point - better results faster! . . VI Different Types of CLR . If you remember, our CyclicalLR callback had different modes, but we didn&#39;t cover them. Now&#39;s the time! . First comes the default mode we&#39;ve already seen - triangular mode. The learning rate just fluctuates between the base and maximum learning rate during the whole process of training: . Triangular2 mode picks up on the idea that learning rate should decrease over time - learning rate difference is halved after every cycle: . And finally, exp range mode picks up on the same idea as previous mode, but decreases learning rate difference exponentially - by an exponential factor of gamma$^{iterations}$: . VII Conclusion . Even though a simple idea, Cyclical Learning Rates introduced both a way to find an optimal range of learning rates and an approach to train neural networks quicker to reach the near-optimal performance. . This can be used to quickly prototype baseline models on new data and get the best possible results at that point. . Who knew fast and better was possible! . . Thank you for reading up to here! . This was mainly made as a reminder and a practice for me, but if this helped you, feel free to share or comment to let me know your thoughts! . If you find any mistakes I made, notify me and I&#39;ll make the necessary changes and mention you and your help. Any and all suggestions and constructive criticism are always welcome. We&#39;re all here to learn! . VIII References and Further Literature . [1] Leslie N. Smith. Cyclical Learning Rates for Training Neural Networks. 2015. U.S. Naval Research Laboratory. [2] Tensorflow&#39;s Custom Keras Callbacks Tutorial [3] Tensorflow&#39;s Tutorial on CNNs (showcases usage of built-in CIFAR-10 dataset) .",
            "url": "https://stopwolf.github.io/blog/optimization/paper/2020/08/28/clr.html",
            "relUrl": "/optimization/paper/2020/08/28/clr.html",
            "date": " • Aug 28, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Understanding Word2Vec: Code and Math",
            "content": ". Text, like everything else in one way or another, can be represented with math and numbers. Words can have meaning even if they aren&#39;t in the textual form us humans are accustomed to. In order to use words to solve problems computationally, researchers had to redefine that approach. And in comes Word2Vec! . Instead of representing words as boring one-hot encoded vectors using only ones and zeros, we could use Word2Vec model and represent each word by dense vectors. Each vector having multitude of features describing every word in its own way. . Instead of leaving this notebook to collect dust in some folder, I wanted to share it. What makes this blog post different than other word2vec from scratch blog posts, is that I&#39;m fully working out the math part, so you (and I) can better understand what&#39;s going on under the hood. . One-Hot Encoded Vectors . Even though I said that one-hot encoded vectors are boring, they are still used in Word2Vec models. That&#39;s why we&#39;ll quickly brush over the topic before moving on. OHE vectors consist only of ones and zeros. The vector is mainly consisted of zeros, except for those indexes where some word $w_c$ (corresponding to the said index) is in a window of words of the target word $w_t$. Don&#39;t bother with this definition. You&#39;ll understand better from the following example. . Example sentence: I like learning everything. . Imagine a word $w_t$ was the word I and the window of words is equal to 2. This window is called context. Practically, this means we are looking two words before the word I and two words after it. Since I is the first word, we can only &#39;see&#39; two words after it. In other words, we can see that the two one-hot encoded vectors corresponding to words like and learning respectively: . I [0] [0] like [1] [0] learning [0] [1] everything [0] [0] . Now, it&#39;s easy to grasp how inefficient this approach would be (memory-wise) if we had thousands or hundreds of thousands of words (some languages have millions) since these vectors are very sparse (they have an extremely low number of non-zero values). That&#39;s where researchers stepped in and revolutionized word representation with the Word2Vec model. Word2Vec has two types of models: . Continuous Bag of Words model (CBOW) | Skip-gram model | . CBOW model is built to predict some target word $w_t$ given words in target word&#39;s window, denoted by $w_c$. For example, given words I have __ every day., the model might predict words like work or classes. . Skip-gram on the other hand is the exact opposite of the CBOW model. Skip-gram uses the target word $w_t$ to predict the words in its context, or $w_c$. For example, given word guitar, the model might predict words like electric, play, acoustic, or bass. The actual word representations come from matrices used in these models. Since there exist two of them, we can just take the mean of elements at the same positions and get one matrix. One of the parameters we can set up is the dimension of the final matrix. Its dimensions will be $V times d$, where $V$ is the number of unique words in our text, or rather the size of the vocabulary, and parameter $d$ represents the dimension of the vector for each word $w_i$. . Utility Functions . Before we begin with the actual word2vec model, we&#39;ll create some utility functions for tokenization, creating one-hot encoded (or OHE for short) vectors, and so on... . The first utility function we&#39;re going to cover is the tokenize function. This function receives some text and extracts all words from it into a list. By using regular expressions, we can create a pattern, which will return an array of all words from the text. . Note: Bellow provided regular expression finds only words, not numbers, and also takes words like &quot;let&#8217;s&quot; and &quot;I&#8217;m&quot; as one word instead of two. . def tokenize(text): pattern = re.compile(r&#39;[A-Za-z]+[ w^ &#39;]*|[ w^ &#39;]*[A-Za-z]+[ w^ &#39;]*&#39;) return pattern.findall(text.lower()) . mapping function creates a vocabulary of unique words and pairs them with numbers for easier manipulation. That&#39;s why we&#39;re using dictionaries mapping words to numbers, but also numbers to words. Since we can&#39;t get keys by the values of a dictionary, we&#39;ll need both dictionaries in the future. . def mapping(text_array): idx_to_word = {} word_to_idx = {} i=0; j=0 for idx, word in enumerate(text_array): if word not in idx_to_word.values(): idx_to_word[i] = word i+=1 if word not in word_to_idx.keys(): word_to_idx[word] = j j+=1 return idx_to_word, word_to_idx . Lastly comes the function for creating OHE vectors. We need to provide tokenized text, dictionary of words to numbers and window size, or rather the context size. Given those, the output will be an array of pairs of OHE vectors. One pair is consisting of OHE vector for $w_t$ (center word), and OHE vectors for $w_c$, one OHE vector for every word in $w_t$&#39;s context. . def one_hot_encoding(text_array, word_to_idx, window_size): matrix = [] for idx, word in enumerate(text_array): center_vec = [0 for w in word_to_idx] center_vec[word_to_idx[word]] = 1 context_vec = [] for i in range(-window_size, window_size+1): if i == 0 or idx+i &lt; 0 or idx+i &gt;= len(text_array) or word == text_array[idx+i]: continue temp = [0 for w in word_to_idx] temp[word_to_idx[text_array[idx+i]]] = 1 context_vec.append(temp) matrix.append([center_vec, context_vec]) return matrix . Training . Since the actual final output of forward pass is the softmax function applied to vector $u$, we&#39;ll first define the softmax function as: $$ frac{exp(u_j)}{ sum_{j&#39;=1}^V exp(u_{j&#39;})}, forall u_j in u$$ By using the softmax function, we transform plain numbers of our output vector $u$ into some probability distribution $p$. That way, we can easily extract words with the highest probabilities of being in a context for $w_t$. . def softmax(u): return [np.exp(u_j) / np.sum(np.exp(u)) for u_j in u] . The forward pass is very simple. It&#39;s comprised of a couple of matrix multiplications and softmax function at the end. . Above is an annotated picture from the paper, so we can better grasp the locations of each variable and why each equation is the way it is. . We can imagine word $w_t$ passing through our model and through following equations: $$ h = w_t^{T} times w_1$$ $$ u = h times w_2$$ and finally: $$ y = mathcal {S}oftmax(u) $$ . def forward_pass(wt, w1, w2): wt = np.array(wt) h = np.dot(wt.T, w1) u = np.dot(h, w2) return softmax(u), h, u . Let&#39;s now work our way through the loss function so we can create a function which calculates it: $$ L = -logP(w_1,..., w_C|w_t) = -log{ displaystyle prod_{c=1}^{C} frac{exp(u_{c,j^*_c})}{ sum_{j&#39;=1}^V exp(u_{j&#39;})}} = - sum_{c=1}^C log frac{exp(u_{j^*_c})}{ sum_{j&#39;=1}^V exp(u_{j&#39;})} = - sum_{c=1}^C(u_{j^*_c} - log sum_{j&#39;=1}^V exp(u_{j&#39;})) = - sum_{c=1}^C u_{j^*_c} + sum_{c=1}^C (log sum_{j&#39;=1}^V exp(u_{j&#39;})) = - sum_{c=1}^C u_{j^*_c} + C log sum_{j&#39;=1}^V exp(u_{j&#39;})$$ The loss function is calculated for every word and after each epoch. $C$ denotes the number of context words for some particular target word. Vector $u$ denotes the output of the forward pass, before the softmax function is applied, which further indicates that $u_{j^*_c}$ is a concrete number in that vector corresponding to the $c$-th context word. Bellow, you can have a look at how we calculate the loss function: . def loss(w_c, u): return -np.sum([u[word.index(1)] for word in w_c]) + len(w_c) * np.log(np.sum(np.exp(u))) . We are taking u[word.index(1)] because our vector word is a one-hot encoded vector, that is, we&#39;re taking the index of that OHE vector where its value is equal to 1 (that gives us the $j_c^*$-th index). . Before we jump to backpropagation, we need to calculate the error the model makes. But why, and how do we do that? . We calculate the error because the error propagates back through the model and that&#39;s ultimately how the model learns. When it comes to &quot;how&quot;, we derive the loss function with respect to every word in the context. . Note: We aren&#8217;t deriving $L = -logP(w_1,..., w_C|w_t)$ but rather loss for one specific word $L = -logP(w_c|w_t)$ like follows: . $$ frac{ partial L}{ partial u_{c,j}} = frac{ partial}{ partial u_{c,j}} (- u_{j_c}) + frac{ partial}{ partial u_{c,j}} log sum_{j&#39;=1}^V exp(u_{j&#39;}) = -t_{c,j} + frac{1}{ sum_{j&#39;=1}^V exp(u_{j&#39;})} frac{ partial}{ partial u_{c,j}} ( sum_{j&#39;=1}^V exp(u_{j&#39;})) = - t_{c,j} + frac{1}{ sum_{j&#39;=1}^V exp(u_{j&#39;})}(0 + ...+ frac{ partial}{ partial u_{c,j}} exp(u_{c, j}) + ... + 0) = frac{exp(u_{c,j})}{ sum_{j&#39;=1}^V exp(u_{j&#39;})} - t_{c,j} = y_{c,j} - t_{c,j}$$ $t_{c,j}$ is OHE vector that represents some word $c$ which has value 1 at $j$-th index; $y_{c,j}$ is $j$-th value of a softmax output of the model for a word $c$. You can look at this as the difference between the model&#39;s output and the expected output. . Now, we sum those errors and calculate the error like so: . def error(out, w_c): out = np.array(out) w_c = np.array(w_c) return np.sum([np.subtract(out, w) for w in w_c], axis=0) . Now that we calculated the error, we need to propagate it back through the model. . Firstly, we calculate the update for $w_2$ matrix: $$ frac{ partial L}{ partial w_2} = sum_{c=1}^C frac{ partial L}{ partial u_{c,j}} frac{ partial u_{c,j}}{ partial w_2} = E cdot h = dw_2$$ This is because $u = h times w_2$ and we already calculated the error from $ frac{ partial L}{ partial u_{c,j}}$. . Before we proceed to the update for $w_1$ matrix, we need to calculate the following derivative (which is similar to the previous equation): $$ frac{ partial L}{ partial h} = sum_{j=1}^V frac{ partial L}{ partial u_{c,j}} frac{ partial u_{c.j}}{ partial h} = E cdot w_2 = EH$$ . Now, we can calculate $dw_1$: $$ frac{ partial L}{ partial w_1} = frac{ partial L}{ partial h} frac{ partial h}{ partial w_1} = EH cdot w_t = dw_1$$ This is because $h = w_t^T times w_1$. . def backprop(out, word, w1, w2, h): err = error(out, word[1]).T dw2 = np.outer(err, h) EH = np.dot(err, w2.T) dw1 = np.outer(EH, np.array(word[0]).T) return dw1.T, dw2.T . The actual part where the model learns to update its weights is defined in the following equation. Mathematically, this is done as: $$ w_1^{new} = w_1^{old} - l_r nabla w_1$$ and same for the second set of weights: $$ w_2^{new} = w_2^{old} - l_r nabla w_2$$ . $ nabla$ (nabla) sign is just a sign for gradient which is why this learning process is called gradient descent. . In our code, we can use $dw_i$ instead of $ nabla w_i$, since they&#39;re one in the same. . def learning(w1, w2, dw1, dw2, lr): w1 = np.subtract(w1, np.multiply(lr, dw1)) w2 = np.subtract(w2, np.multiply(lr, dw2)) return w1, w2 . Now, in the train method, we collect everything previously done into one function. We initialize weights with random uniform distribution ${ mathcal {U}}(-1, 1)$. Since these numbers may be too large, we multiply them with 0.01. . In every epoch, for every word, we perform forward pass, backpropagation, and then update weights and calculate the loss. history is set up as a way to monitor (and later plot) losses as the model goes through each epoch. This is mainly done to check if the model is learning something or not. . def train(words, vocab, lr, epochs, dim): w1 = np.random.uniform(-1, 1, (len(vocab), dim)) * 0.01 w2 = np.random.uniform(-1, 1, (dim, len(vocab))) * 0.01 history = {} for e in range(epochs): l = 0 for word in words: out, h, u = forward_pass(word[0], w1, w2) dw1, dw2 = backprop(out, word, w1, w2, h) w1, w2 = learning(w1, w2, dw1, dw2, lr) l += loss(word[1], u) if e % 100 == 0: history[e] = l return w1, w2, history . Predicting . Predicting is nothing more than a forward pass through the model for some target word and then returning the most likely ones to be in its context. . Note: We can&#8217;t provide a word the model wasn&#8217;t trained on, no matter how similar or synonymous the word is to those words we did train our model on. . def predict(word, w1, w2, matrix, words, indices): word = word.lower() out, _, _ = forward_pass(matrix[words[word]][0], w1, w2) most_likely_idxs = np.array(out).argsort()[-4:][::-1] return [indices[w] for w in most_likely_idxs] . Going into practice . Let&#39;s test our Word2Vec model and see how everything works. We&#39;ll take a random sentence as an example to train the model on. . All this is done in couple of steps: . Tokenize the text | Map tokens to numbers | Create appropriate OHE vectors | Train the model | text = &#39;I like reading The Witcher books every night, but I only have one left in the series.&#39; tokens = tokenize(text) idxs, words = mapping(tokens) mat = one_hot_encoding(tokens, words, window_size=2) w1, w2, history = train(mat, words, lr=0.01, epochs=1000, dim=5) . We&#39;ll plot the loss function over epochs during training to see if the model learned got better over time at predicting the context words given the target words. . plt.figure(figsize=(13, 8)) plt.plot(list(history.keys()), list(history.values()), color=&#39;red&#39;) plt.xlabel(&#39;Numer of epochs&#39;) plt.ylabel(&#39;Loss&#39;) . Text(0, 0.5, &#39;Loss&#39;) . And it did! The loss was reducing at rapid speed, and then plateaued (or reached the point of no big changes in learning). . We can also have a look at our embeddings, or word representations, which may be the main result of this lesson. You can think of each row as one word from our example sentence, and each column as a feature. . array([[ 0.47 , -1.7471, 0.1557, 0.4786, 1.4846], [-0.9634, -1.435 , -0.2449, -0.2901, -0.0538], [-1.4122, -1.4743, -0.4483, -0.204 , -0.1574], [-1.117 , 0.2088, -0.9444, -1.2663, -1.1992], [-1.9791, 0.51 , -0.4437, -0.3276, 0.5801], [-1.7116, 0.9246, 0.2479, -0.6965, 0.9689], [-1.4144, 1.4498, 0.3125, 0.2373, 1.2433], [-0.6107, 0.2046, 0.8176, -0.0828, 2.0107], [ 0.1517, 0.8335, 1.1067, 0.9358, 1.9614], [ 1.565 , -0.3988, 1.1101, 1.2965, 0.6315], [ 1.9103, -0.4376, -0.0585, 0.3374, -0.311 ], [ 2.0972, -0.3943, 0.3935, 0.3768, -0.918 ], [ 1.2966, 0.0654, -0.6747, -0.2323, -1.4494], [ 1.3057, -0.0296, -0.8275, -0.7703, -1.7792], [ 0.2715, 0.2859, -0.9785, -1.126 , -1.8184]]) . The prediction part of the model shouldn&#39;t be forgotten either. If we predict context words of the word Witcher for example, we&#39;ll get the following most likely context words: . Note: Example sentence was: I like reading The Witcher books every night, but I only have one left in the series. . predict(&#39;Witcher&#39;, w1, w2, mat, words, idxs) . [&#39;the&#39;, &#39;books&#39;, &#39;reading&#39;, &#39;every&#39;] . Conclusion . Working with words was inefficient and slow. One-hot encoded vectors were an easy and quick solution but included no intuitiveness or creativity. Until Word2Vec appeared as a seemingly simple solution which was its own revolution. It changed how we observe, process, and work with words and texts in general. . . . Thank you for reading up to here! This was mainly made as a reminder and a practice for me, but if this helped you, feel free to share or comment to let me know your thoughts! . If you find any mistakes I made, notify me and I&#39;ll make the necessary changes and mention you and your help. Any and all suggestions and constructive criticism are always welcome. We&#39;re all here to learn! . References And Further Literature . [1] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. ICLR Workshop, 2013. [2] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of Words and Phrases and their Compositionality. Accepted to NIPS 2013. [3] X. Rong. word2vec Parameter Learning Explained. ArXiv 2014. .",
            "url": "https://stopwolf.github.io/blog/nlp/paper/2020/08/17/word2vec.html",
            "relUrl": "/nlp/paper/2020/08/17/word2vec.html",
            "date": " • Aug 17, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://stopwolf.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://stopwolf.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I’m a third year student studying Information Systems and Technology at Faculty of Organizational Sciences in Belgrade, Serbia. . When I’m not studying for my exams, I’m working part-time for Saga NFG as ML Programmer, creating ML-based solutions for the analytical module of Selecta CRM software. Have a look here! . I’m also interested in biology and medicine and would like to explore the overlap of the two with Machine Learning. . In my actual free time I like to play basketball, meditate, workout, read and learn new stuff. .",
          "url": "https://stopwolf.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "",
          "content": "Welcome to my humble blog, where I post about machine learning topics I recently learned about! I made this mainly to fuel my will to learn and share new things with the world. . Next topics I’m currently working on are: . Concolutional Neural Networks that’ll be able to say “I don’t know” | Predicting next-game stats of NBA players (LeBron James to be exact) | How to deploy machine learning models | . Note that these ideas change all the time and that they might not make it to blog post form.. . If you’re interested in hot this site is built, the answer is with fastpages! fastpages automates the process of creating blog posts via GitHub Actions, so you don’t have to fuss with conversion scripts. The best part is that you can directly post your Jupyter Notebooks as blog posts! . A full list of features can be found on GitHub. . Posts .",
          "url": "https://stopwolf.github.io/blog/",
          "relUrl": "/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://stopwolf.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}